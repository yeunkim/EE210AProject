{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib inline\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8\n",
      "  Looking up \"https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz\" in the cache\n",
      "  Current age based on date: 68\n",
      "  Freshness lifetime from max-age: 31557600\n",
      "  The response is \"fresh\", returning cached response\n",
      "  31557600 > 68\n",
      "  Using cached xlrd-0.9.0.tar.gz\n",
      "  Downloading from URL https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8\n",
      "  Running setup.py (path:/tmp/pip-ta3y2ecq-build/setup.py) egg_info for package from https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8\n",
      "    Running command python setup.py egg_info\n",
      "    running egg_info\n",
      "    creating pip-egg-info/xlrd.egg-info\n",
      "    writing dependency_links to pip-egg-info/xlrd.egg-info/dependency_links.txt\n",
      "    writing top-level names to pip-egg-info/xlrd.egg-info/top_level.txt\n",
      "    writing pip-egg-info/xlrd.egg-info/PKG-INFO\n",
      "    writing manifest file 'pip-egg-info/xlrd.egg-info/SOURCES.txt'\n",
      "    reading manifest file 'pip-egg-info/xlrd.egg-info/SOURCES.txt'\n",
      "    writing manifest file 'pip-egg-info/xlrd.egg-info/SOURCES.txt'\n",
      "  Source in /tmp/pip-ta3y2ecq-build has version 0.9.0, which satisfies requirement xlrd==0.9.0 from https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8\n",
      "Building wheels for collected packages: xlrd\n",
      "  Running setup.py bdist_wheel for xlrd ... \u001b[?25l  Destination directory: /tmp/tmp_cs4pnq5pip-wheel-\n",
      "  Running command /root/miniconda3/envs/carnd-term1/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-ta3y2ecq-build/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/tmp_cs4pnq5pip-wheel- --python-tag cp35\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib\n",
      "  creating build/lib/xlrd\n",
      "  copying xlrd/info.py -> build/lib/xlrd\n",
      "  copying xlrd/timemachine.py -> build/lib/xlrd\n",
      "  copying xlrd/formatting.py -> build/lib/xlrd\n",
      "  copying xlrd/sheet.py -> build/lib/xlrd\n",
      "  copying xlrd/xldate.py -> build/lib/xlrd\n",
      "  copying xlrd/__init__.py -> build/lib/xlrd\n",
      "  copying xlrd/biffh.py -> build/lib/xlrd\n",
      "  copying xlrd/formula.py -> build/lib/xlrd\n",
      "  copying xlrd/xlsx.py -> build/lib/xlrd\n",
      "  copying xlrd/compdoc.py -> build/lib/xlrd\n",
      "  copying xlrd/book.py -> build/lib/xlrd\n",
      "  copying xlrd/licences.py -> build/lib/xlrd\n",
      "  creating build/lib/xlrd/doc\n",
      "  copying xlrd/doc/xlrd.html -> build/lib/xlrd/doc\n",
      "  copying xlrd/doc/compdoc.html -> build/lib/xlrd/doc\n",
      "  creating build/lib/xlrd/examples\n",
      "  copying xlrd/examples/namesdemo.xls -> build/lib/xlrd/examples\n",
      "  copying xlrd/examples/xlrdnameAPIdemo.py -> build/lib/xlrd/examples\n",
      "  running build_scripts\n",
      "  creating build/scripts-3.5\n",
      "  copying and adjusting scripts/runxlrd.py -> build/scripts-3.5\n",
      "  changing mode of build/scripts-3.5/runxlrd.py from 644 to 755\n",
      "  installing to build/bdist.linux-x86_64/wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating build/bdist.linux-x86_64\n",
      "  creating build/bdist.linux-x86_64/wheel\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/info.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd/doc\n",
      "  copying build/lib/xlrd/doc/xlrd.html -> build/bdist.linux-x86_64/wheel/xlrd/doc\n",
      "  copying build/lib/xlrd/doc/compdoc.html -> build/bdist.linux-x86_64/wheel/xlrd/doc\n",
      "  copying build/lib/xlrd/timemachine.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/formatting.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/sheet.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/xldate.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/__init__.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/biffh.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/formula.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/xlsx.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/compdoc.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd/examples\n",
      "  copying build/lib/xlrd/examples/namesdemo.xls -> build/bdist.linux-x86_64/wheel/xlrd/examples\n",
      "  copying build/lib/xlrd/examples/xlrdnameAPIdemo.py -> build/bdist.linux-x86_64/wheel/xlrd/examples\n",
      "  copying build/lib/xlrd/book.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/licences.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  running install_egg_info\n",
      "  running egg_info\n",
      "  creating xlrd.egg-info\n",
      "  writing top-level names to xlrd.egg-info/top_level.txt\n",
      "  writing xlrd.egg-info/PKG-INFO\n",
      "  writing dependency_links to xlrd.egg-info/dependency_links.txt\n",
      "  writing manifest file 'xlrd.egg-info/SOURCES.txt'\n",
      "  reading manifest file 'xlrd.egg-info/SOURCES.txt'\n",
      "  writing manifest file 'xlrd.egg-info/SOURCES.txt'\n",
      "  Copying xlrd.egg-info to build/bdist.linux-x86_64/wheel/xlrd-0.9.0-py3.5.egg-info\n",
      "  running install_scripts\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd-0.9.0.data\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd-0.9.0.data/scripts\n",
      "  copying build/scripts-3.5/runxlrd.py -> build/bdist.linux-x86_64/wheel/xlrd-0.9.0.data/scripts\n",
      "  changing mode of build/bdist.linux-x86_64/wheel/xlrd-0.9.0.data/scripts/runxlrd.py to 755\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd-0.9.0.dist-info/WHEEL\n",
      "  creating '/tmp/tmp_cs4pnq5pip-wheel-/xlrd-0.9.0-cp35-none-any.whl' and adding '.' to it\n",
      "  adding 'xlrd/__init__.py'\n",
      "  adding 'xlrd/biffh.py'\n",
      "  adding 'xlrd/book.py'\n",
      "  adding 'xlrd/compdoc.py'\n",
      "  adding 'xlrd/formatting.py'\n",
      "  adding 'xlrd/formula.py'\n",
      "  adding 'xlrd/info.py'\n",
      "  adding 'xlrd/licences.py'\n",
      "  adding 'xlrd/sheet.py'\n",
      "  adding 'xlrd/timemachine.py'\n",
      "  adding 'xlrd/xldate.py'\n",
      "  adding 'xlrd/xlsx.py'\n",
      "  adding 'xlrd/doc/compdoc.html'\n",
      "  adding 'xlrd/doc/xlrd.html'\n",
      "  adding 'xlrd/examples/namesdemo.xls'\n",
      "  adding 'xlrd/examples/xlrdnameAPIdemo.py'\n",
      "  adding 'xlrd-0.9.0.data/scripts/runxlrd.py'\n",
      "  adding 'xlrd-0.9.0.dist-info/DESCRIPTION.rst'\n",
      "  adding 'xlrd-0.9.0.dist-info/metadata.json'\n",
      "  adding 'xlrd-0.9.0.dist-info/top_level.txt'\n",
      "  adding 'xlrd-0.9.0.dist-info/WHEEL'\n",
      "  adding 'xlrd-0.9.0.dist-info/METADATA'\n",
      "  adding 'xlrd-0.9.0.dist-info/RECORD'\n",
      "done\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f4/0c/09/39986983b4a793686e129a320cc5c39cf14f6ce6067f961a8d\n",
      "  Removing source in /tmp/pip-ta3y2ecq-build\n",
      "Successfully built xlrd\n",
      "Installing collected packages: xlrd\n",
      "\n",
      "Successfully installed xlrd-0.9.0\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!pip install -Iv https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/pandas/util/_decorators.py:118: FutureWarning: The `sheetname` keyword is deprecated, use `sheet_name` instead\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import pandas as pd\n",
    "import os \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "demographics = pd.read_excel('/src/Data/ANIML/oasis_cross-sectional.xls', sheetname=3) # load data\n",
    "df = demographics.dropna(how='any') # remove NaN values\n",
    "df_columns = list(demographics.columns)\n",
    "X_columns = np.delete(df_columns, [6,7], None) # X matrix won't have MMSE or CDR scores\n",
    "Xdf = df.reindex(columns=X_columns)\n",
    "X = Xdf.values # creating X\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "# Define function to get list of pngs based on slice number\n",
    "pngs_path='/src/Data/ANIML/OASIS_pngs'\n",
    "def getaxialPNG(path):\n",
    "    l = []\n",
    "    axialslice90_files = []\n",
    "    axialslice91_files = []\n",
    "    axialslice92_files = []\n",
    "    axialslice93_files = []\n",
    "    axialslice94_files = []\n",
    "    axialslice95_files = []\n",
    "    axialslice96_files = []\n",
    "    axialslice97_files = []\n",
    "    axialslice98_files = []\n",
    "    axialslice99_files = []\n",
    "    \n",
    "    for root, directories, filenames in os.walk(path):\n",
    "        \n",
    "        for filename in filenames:\n",
    "            if \".90.\" in filename: \n",
    "                axialslice90_files.append(os.path.join(root, filename))\n",
    "            if \".91.\" in filename: \n",
    "                axialslice91_files.append(os.path.join(root, filename))\n",
    "            if \".92.\" in filename: \n",
    "                axialslice92_files.append(os.path.join(root, filename))\n",
    "            if \".93.\" in filename: \n",
    "                axialslice93_files.append(os.path.join(root, filename))\n",
    "            if \".94.\" in filename: \n",
    "                axialslice94_files.append(os.path.join(root, filename))\n",
    "            if \".95.\" in filename: \n",
    "                axialslice95_files.append(os.path.join(root, filename))\n",
    "            if \".96.\" in filename: \n",
    "                axialslice96_files.append(os.path.join(root, filename))\n",
    "            if \".97.\" in filename: \n",
    "                axialslice97_files.append(os.path.join(root, filename))\n",
    "            if \".98.\" in filename: \n",
    "                axialslice98_files.append(os.path.join(root, filename))\n",
    "            if \".99.\" in filename: \n",
    "                axialslice99_files.append(os.path.join(root, filename))\n",
    "\n",
    "    l = list(zip(axialslice90_files, axialslice91_files, axialslice92_files, axialslice93_files, axialslice94_files, axialslice95_files, axialslice96_files, axialslice97_files, axialslice98_files, axialslice99_files))\n",
    "\n",
    "    return ((np.asarray(l)))\n",
    "\n",
    "axial_files0 = getaxialPNG(pngs_path)\n",
    "axial_X_files = np.take(axial_files0, indices=df.index.values, axis=0) # keeps the images with the same index as X matrix\n",
    "axial90loc, axial91loc, axial92loc, axial93loc, axial94loc, axial95loc, axial96loc, axial97loc, axial98loc, axial99loc = zip(*axial_X_files)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "X_id, X_sex, X_handedness, X_age, X_education, X_SES, X_eTIV, X_nWBV, X_ASF = zip(*X) # unzips big X matrix\n",
    "\n",
    "def sex_translator(X_sex):\n",
    "    X_sex_binary = []\n",
    "    X_sex_encoded = []\n",
    "    for x in X_sex:\n",
    "        if x == 'M':\n",
    "            X_sex_binary.append(1)\n",
    "            X_sex_encoded.append([0,1])\n",
    "        else:\n",
    "            X_sex_binary.append(-1)\n",
    "            X_sex_encoded.append([1,0])\n",
    "    \n",
    "    return(zip(X_sex_binary, X_sex_encoded)) # gives us binary and one-hot encoded for sex\n",
    "           \n",
    "def hand_translator(X_handedness):\n",
    "    X_hand_binary = []\n",
    "    X_hand_encoded = []\n",
    "    for x in X_handedness:\n",
    "        if x == 'R':\n",
    "            X_hand_binary.append(1)\n",
    "            X_hand_encoded.append([0,1])\n",
    "        else:\n",
    "            X_hand_binary.append(-1)\n",
    "            X_hand_encoded.append([1,0])\n",
    "    \n",
    "    return(zip(X_hand_binary, X_hand_encoded)) # same as above but for handedness\n",
    "\n",
    "# turns out all the patients are right-handed\n",
    "# maybe we should all just be left-handed so we don't suffer from Alzheimer's\n",
    "# /frequentist sarcasm\n",
    "\n",
    "X_sex_binary, X_sex_encoded = zip(*sex_translator(X_sex)) # unzipping to get our function outputs\n",
    "X_hand_binary, X_hand_encoded = zip(*hand_translator(X_handedness))\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "def prepPNGimgs(array_of_image_paths):\n",
    "    l = []\n",
    "    for img_file in array_of_image_paths: # for each file in the list of images...\n",
    "        img = cv2.imread(\"{}\".format(img_file)) # read the image...\n",
    "        img = cv2.resize(np.array(img), (224,224)) # resize it to 224 by 224 QUICK WHAT'S 224 SQUARED??\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # makes it grayscale\n",
    "        flatten = gray.flatten() # spaghettifies it into 1 by 50176 (which is 224 squared)\n",
    "        l.append(flatten)\n",
    "    \n",
    "    return(np.asarray(l))\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "Y_CDR_columns = [column_name for column_name in df_columns if column_name == 'CDR']\n",
    "Y_CDR_df = df.reindex(columns=Y_CDR_columns)\n",
    "Y_CDR = Y_CDR_df.values\n",
    "\n",
    "Y_MMSE_columns = [column_name for column_name in df_columns if column_name == 'MMSE']\n",
    "Y_MMSE_df = df.reindex(columns=Y_MMSE_columns)\n",
    "Y_MMSE = Y_MMSE_df.values\n",
    "\n",
    "CDR_threshold_0 = 0 # threshold values by CDR scale\n",
    "CDR_threshold_0point5 = 0.5\n",
    "CDR_threshold_1 = 1\n",
    "\n",
    "MMSE_threshold_24 = 24 # threshold values by MMSE scale\n",
    "MMSE_threshold_18 = 18\n",
    "\n",
    "def CDR_probable_AD_thresholder(Y_CDR, threshold_value):\n",
    "    Y_CDR_binary = []\n",
    "    Y_CDR_encoded = []\n",
    "    for y in Y_CDR:\n",
    "        if y > threshold_value:\n",
    "            Y_CDR_binary.append(1)\n",
    "            Y_CDR_encoded.append([0,1])\n",
    "        else:\n",
    "            Y_CDR_binary.append(-1)\n",
    "            Y_CDR_encoded.append([1,0])\n",
    "\n",
    "    return((zip(Y_CDR_binary, Y_CDR_encoded)))\n",
    "\n",
    "def MMSE_probable_Dementia_thresholder(Y_MMSE, threshold_value):\n",
    "    Y_MMSE_binary = []\n",
    "    Y_MMSE_encoded = []\n",
    "    for y in Y_MMSE:\n",
    "        if y < threshold_value:\n",
    "            Y_MMSE_binary.append(1)\n",
    "            Y_MMSE_encoded.append([0,1])\n",
    "        else:\n",
    "            Y_MMSE_binary.append(-1)\n",
    "            Y_MMSE_encoded.append([1,0])\n",
    "        \n",
    "    return(zip(Y_MMSE_binary, Y_MMSE_encoded))\n",
    "\n",
    "Y_CDR_binary, Y_CDR_encoded = zip(*CDR_probable_AD_thresholder(Y_CDR, CDR_threshold_0))\n",
    "Y_MMSE_binary, Y_MMSE_encoded = zip(*MMSE_probable_Dementia_thresholder(Y_MMSE, MMSE_threshold_24))\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "# turning everything into numpy arrays because why not\n",
    "\n",
    "df_index = np.asarray(df.index.values)\n",
    "X_id = np.asarray(X_id)\n",
    "X_sex = np.asarray(X_sex)\n",
    "X_sex_binary = np.asarray(X_sex_binary)\n",
    "X_sex_encoded = np.asarray(X_sex_encoded)\n",
    "X_handedness = np.asarray(X_handedness)\n",
    "X_hand_binary = np.asarray(X_hand_binary)\n",
    "X_hand_encoded = np.asarray(X_hand_encoded)\n",
    "X_age = np.asarray(X_age) \n",
    "X_education = np.asarray(X_education) \n",
    "X_SES = np.asarray(X_SES)\n",
    "X_eTIV = np.asarray(X_eTIV)\n",
    "X_nWBV = np.asarray(X_nWBV)\n",
    "X_ASF = np.asarray(X_ASF)\n",
    "axial90loc = np.asarray(axial90loc) \n",
    "axial91loc = np.asarray(axial91loc)\n",
    "axial92loc = np.asarray(axial92loc) \n",
    "axial93loc = np.asarray(axial93loc)\n",
    "axial94loc = np.asarray(axial94loc)\n",
    "axial95loc = np.asarray(axial95loc) \n",
    "axial96loc = np.asarray(axial96loc) \n",
    "axial97loc = np.asarray(axial97loc) \n",
    "axial98loc = np.asarray(axial98loc) \n",
    "axial99loc = np.asarray(axial99loc)\n",
    "axial90_spaghetti = prepPNGimgs(axial90loc)\n",
    "axial91_spaghetti = prepPNGimgs(axial91loc)\n",
    "axial92_spaghetti = prepPNGimgs(axial92loc)\n",
    "axial93_spaghetti = prepPNGimgs(axial93loc)\n",
    "axial94_spaghetti = prepPNGimgs(axial94loc)\n",
    "axial95_spaghetti = prepPNGimgs(axial95loc)\n",
    "axial96_spaghetti = prepPNGimgs(axial96loc)\n",
    "axial97_spaghetti = prepPNGimgs(axial97loc)\n",
    "axial98_spaghetti = prepPNGimgs(axial98loc)\n",
    "axial99_spaghetti = prepPNGimgs(axial99loc)\n",
    "Y_CDR = np.squeeze(np.asarray(Y_CDR))\n",
    "Y_CDR_binary = np.asarray(Y_CDR_binary)\n",
    "Y_CDR_encoded = np.asarray(Y_CDR_encoded)\n",
    "Y_MMSE = np.squeeze(np.asarray(Y_MMSE))\n",
    "Y_MMSE_binary = np.asarray(Y_MMSE_binary)\n",
    "Y_MMSE_encoded = np.asarray(Y_MMSE_encoded)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "FinalX = np.vstack((X_id, X_sex, X_sex_binary, X_handedness, X_hand_binary, X_age, X_education, X_SES, X_eTIV, X_nWBV, X_ASF))\n",
    "FinalX = FinalX.T\n",
    "\n",
    "FinalY = np.vstack((Y_CDR, Y_CDR_binary, Y_MMSE, Y_MMSE_binary))\n",
    "FinalY = FinalY.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "axial_X_files\n",
    "axial_files0.shape\n",
    "# np.savetxt(\"OASIS_subjs.csv\", axial90loc, delimiter=\",\", fmt='<U61')\n",
    "dt = pd.DataFrame(axial90loc)\n",
    "dt.to_csv(\"OASIS_subjs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "XDATAFRAME = pd.DataFrame((FinalX), index=df_index, columns=['X_id', 'X_sex', 'X_sex_binary', 'X_handedness', 'X_hand_binary', 'X_age', 'X_education', 'X_SES', 'X_eTIV', 'X_nWBV', 'X_ASF'])\n",
    "\n",
    "YDATAFRAME = pd.DataFrame((FinalY), index=df_index, columns=['Y_CDR', 'Y_CDR_binary', 'Y_MMSE', 'Y_MMSE_binary'])\n",
    "\n",
    "classifiers = pd.DataFrame((Y_CDR_encoded), index=df_index, columns=['NC','AD'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "axial90s = pd.concat([pd.DataFrame((axial90_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial91s = pd.concat([pd.DataFrame((axial91_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial92s = pd.concat([pd.DataFrame((axial92_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial93s = pd.concat([pd.DataFrame((axial93_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial94s = pd.concat([pd.DataFrame((axial94_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial95s = pd.concat([pd.DataFrame((axial95_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial96s = pd.concat([pd.DataFrame((axial96_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial97s = pd.concat([pd.DataFrame((axial97_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial98s = pd.concat([pd.DataFrame((axial98_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial99s = pd.concat([pd.DataFrame((axial99_spaghetti), index=df_index), classifiers], axis=1)\n",
    "\n",
    "# axial91s = pd.DataFrame((axial91_spaghetti), index=df_index)\n",
    "# axial92s = pd.DataFrame((axial92_spaghetti), index=df_index)\n",
    "# axial93s = pd.DataFrame((axial93_spaghetti), index=df_index)\n",
    "# axial94s = pd.DataFrame((axial94_spaghetti), index=df_index)\n",
    "# axial95s = pd.DataFrame((axial95_spaghetti), index=df_index)\n",
    "# axial96s = pd.DataFrame((axial96_spaghetti), index=df_index)\n",
    "# axial97s = pd.DataFrame((axial97_spaghetti), index=df_index)\n",
    "# axial98s = pd.DataFrame((axial98_spaghetti), index=df_index)\n",
    "# axial99s = pd.DataFrame((axial99_spaghetti), index=df_index)\n",
    "\n",
    "# divide subject groups into training and test\n",
    "\n",
    "\n",
    "# allSlices = pd.concat([axial90s, axial91s, axial93s, axial94s, axial95s, axial96s, axial97s, axial98s, axial99s],axis=0)\n",
    "\n",
    "# #shuffle rows\n",
    "# allSlicesShuff = allSlices.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# X_train0 = allSlicesShuff.drop(['NC', 'AD'], axis=1)[0:1000].as_matrix()\n",
    "# y_train0 = allSlicesShuff['AD'][0:1000].as_matrix()\n",
    "\n",
    "# X_test0 = allSlicesShuff.drop(['NC', 'AD'], axis=1)[1000:].as_matrix()\n",
    "# y_test0 = allSlicesShuff['AD'][1000:].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 50178)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axial90s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# axial90s.shape\n",
    "ind= range(0,216)\n",
    "random.seed(1)\n",
    "train_ind = random.sample(range(0,216),200)\n",
    "\n",
    "axials_train = {}\n",
    "axials_train[1] = axial90s.take(train_ind)\n",
    "axials_train[2] = axial91s.take(train_ind)\n",
    "axials_train[3] = axial92s.take(train_ind)\n",
    "axials_train[4] = axial93s.take(train_ind)\n",
    "axials_train[5] = axial94s.take(train_ind)\n",
    "axials_train[6] = axial95s.take(train_ind)\n",
    "axials_train[7] = axial96s.take(train_ind)\n",
    "axials_train[8] = axial97s.take(train_ind)\n",
    "axials_train[9] = axial98s.take(train_ind)\n",
    "axials_train[10] = axial99s.take(train_ind)\n",
    "\n",
    "\n",
    "test_ind = list(set(ind) - set(train_ind))\n",
    "\n",
    "axials_test={}\n",
    "axials_test[1] = axial90s.take(test_ind)\n",
    "axials_test[2] = axial91s.take(test_ind)\n",
    "axials_test[3] = axial92s.take(test_ind)\n",
    "axials_test[4] = axial93s.take(test_ind)\n",
    "axials_test[5] = axial94s.take(test_ind)\n",
    "axials_test[6] = axial95s.take(test_ind)\n",
    "axials_test[7] = axial96s.take(test_ind)\n",
    "axials_test[8] = axial97s.take(test_ind)\n",
    "axials_test[9] = axial98s.take(test_ind)\n",
    "axials_test[10] = axial99s.take(test_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pd.concat(axials_train)\n",
    "testDF = pd.concat(axials_test)\n",
    "\n",
    "X_train0 = trainDF.drop(['NC', 'AD'], axis=1).as_matrix()\n",
    "y_train0 = trainDF['AD'].as_matrix()\n",
    "\n",
    "X_test0 = testDF.drop(['NC', 'AD'], axis=1).as_matrix()\n",
    "y_test0 = testDF['AD'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train0)\n",
    "y_train0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(X_train0[1].reshape(224,224), cmap='gray')\n",
    "def cannyEdge(imgArray):\n",
    "    edgesList = []\n",
    "    for i in range(0, imgArray.shape[0]):\n",
    "        img = imgArray[i].reshape(224,224)\n",
    "\n",
    "        nz = np.nonzero(img)\n",
    "\n",
    "        lower = np.mean(nz)*0.25\n",
    "        upper = np.mean(nz)*0.75\n",
    "\n",
    "        edges= cv2.Canny(img, lower, upper)\n",
    "        flatEdge = edges.flatten()\n",
    "        edgesList.append(flatEdge)\n",
    "\n",
    "    return(np.asarray(edgesList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_edges = cannyEdge(X_train0)\n",
    "X_test_edges = cannyEdge(X_test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath= '/src/Data/ANIML/'\n",
    "np.save('/src/Data/ANIML/X_train.npy', X_train0)\n",
    "np.save(dataPath+'y_train.npy', y_train0)\n",
    "np.save(dataPath+'X_test.npy', X_test0)\n",
    "np.save(dataPath+'y_test.npy', y_test0)\n",
    "np.save(dataPath+'AX90.npy', axial90s)\n",
    "np.save(dataPath+'AX91.npy', axial91s)\n",
    "np.save(dataPath+'AX92.npy', axial92s)\n",
    "np.save(dataPath+'AX93.npy', axial93s)\n",
    "np.save(dataPath+'AX94.npy', axial94s)\n",
    "np.save(dataPath+'AX95.npy', axial95s)\n",
    "np.save(dataPath+'AX96.npy', axial96s)\n",
    "np.save(dataPath+'AX97.npy', axial97s)\n",
    "np.save(dataPath+'AX98.npy', axial98s)\n",
    "np.save(dataPath+'AX99.npy', axial99s)\n",
    "np.save(dataPath+'X_train_edges.npy', X_train_edges)\n",
    "np.save(dataPath+'X_test_edges.npy', X_test_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Model # basic class for specifying and training a neural network\n",
    "from keras.layers import MaxPooling2D,Input, Convolution2D, Dense, Dropout, Flatten, Concatenate, Merge, merge\n",
    "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values\n",
    "from load_data import load\n",
    "from metadata import prepare_metadata\n",
    "###### get IDs and labels ######\n",
    "# fn = '/Users/newhaven510/Desktop/UCLA_Spring_18/EE210A/OASIS3_noNA_ybin.csv'\n",
    "fn = '/Users/newhaven510/Downloads/new_Y_Data.csv'\n",
    "\n",
    "dirimg = '/Users/newhaven510/Desktop/UCLA_Spring_18/EE210A/Original_cropped/'\n",
    "dirjac = '/Users/newhaven510/Desktop/UCLA_Spring_18/EE210A/Jac_cropped/'\n",
    "data = load(fn, dirimg, dirjac)\n",
    "Y_train0 = data.y_train0\n",
    "Y_test0 = data.y_test0\n",
    "###### smooth and normalize ######\n",
    "X_train0 = data.train_data_matrix[:,61,:,:]\n",
    "X_test0 = data.test_data_matrix[:,61,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### set data #######\n",
    "X_train, Y_train = X_train0, Y_train0\n",
    "X_train= X_train.astype('float32')\n",
    "# for i in range(0,X_train.shape[0]):\n",
    "#     X_train[i,:,:] /= X_train[i,:,:].max()\n",
    "X_train/=255\n",
    "\n",
    "X_test, Y_test = X_test0, Y_test0\n",
    "X_test = X_test.astype('float32')\n",
    "# for i in range(0,X_test.shape[0]):\n",
    "#     X_test[i,:,:] /= X_test[i,:,:].max()\n",
    "X_test/=255\n",
    "X_train = np.expand_dims(X_train, axis =3)\n",
    "X_test = np.expand_dims(X_test, axis =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Model Functional\n",
    "number_of_classes = 2\n",
    "Y_train = np_utils.to_categorical(Y_train, number_of_classes)\n",
    "Y_train = Y_train.astype('float32')\n",
    "Y_test = np_utils.to_categorical(Y_test, number_of_classes)\n",
    "Y_test = Y_test.astype('float32')\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train,Y_train = shuffle(X_train,Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.6'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "\n",
    "class LeNetMnistClassifier():\n",
    "    @staticmethod\n",
    "    def toDigit(hot_encode):\n",
    "        return np.argmax(hot_encode)\n",
    "        \n",
    "    def __init__(self, **kwargs):\n",
    "        if kwargs.get(\"model_path\", None):\n",
    "            from keras.models import load_model\n",
    "            self.model = load_model(kwargs['model_path'])\n",
    "        else:\n",
    "            self.activation_function = kwargs.pop('activation_function', 'relu')\n",
    "            self.batch_size = kwargs.pop('batch_size', 15)\n",
    "            self.epochs = kwargs.pop('epochs', 5)\n",
    "            self.kernal_size = kwargs.pop('kernal_size', (3, 3))\n",
    "            self.loss_function = kwargs.pop('loss_function', 'categorical_crossentropy')\n",
    "            self.optimizer = kwargs.pop('optimizer', 'sgd')\n",
    "            self.pool_size = kwargs.pop('pool_size', (2, 2))\n",
    "            self.model = self._model(X_train)#, X_train2)\n",
    "            \n",
    "    \n",
    "    def _model(self, X_train):#, X_train2):\n",
    "        seed(1)\n",
    "        model = Sequential()#, X_train2])\n",
    "#         Model\n",
    "        model.add(Conv2D(32, kernel_size=self.kernal_size, padding='same',\n",
    "                 activation=self.activation_function,\n",
    "                 input_shape=(64,64,1)))\n",
    "        # One additional convolutional layer (32 channels)\n",
    "        model.add(Conv2D(32, kernel_size=self.kernal_size, padding='same',\n",
    "                 activation=self.activation_function))\n",
    "        model.add(Conv2D(32, kernel_size=self.kernal_size, padding='same',\n",
    "                 activation=self.activation_function))\n",
    "        model.add(MaxPooling2D(pool_size=self.pool_size))\n",
    "        model.add(Conv2D(64, self.kernal_size, padding='same', activation=self.activation_function))\n",
    "        # One additional convolutional layer (64 channels)\n",
    "        model.add(Conv2D(64, self.kernal_size, padding='same', activation=self.activation_function))\n",
    "        model.add(Conv2D(64, self.kernal_size, padding='same', activation=self.activation_function))\n",
    "        model.add(MaxPooling2D(pool_size=self.pool_size))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation=self.activation_function))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def predict(self, digits=()):\n",
    "        return self.model.predict(digits)\n",
    "    \n",
    "    def evaluate(self, X_test=None, y_test=None):\n",
    "        X_test = X_test.reshape(X_test.shape[0], 224, 224, 1)\n",
    "        X_test = X_test.astype('float32')\n",
    "        X_test/=255        \n",
    "        number_of_classes = 2\n",
    "        y_test = np_utils.to_categorical(y_test, number_of_classes)\n",
    "        return self.model.evaluate(X_test, y_test)\n",
    "    \n",
    "    def preprocess_and_train(self, X_train=None, y_train=None, X_test=None,y_test=None):\n",
    "        self._train(X_train,  y_train, X_test, y_test)\n",
    "        \n",
    "    def _preprocess(self, X_train, X_train2, y_train, X_test, X_test2,y_test):\n",
    "        X_train = X_train.reshape(X_train.shape[0], 224, 224, 1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], 224, 224, 1)\n",
    "\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_test = X_test.astype('float32')\n",
    "        \n",
    "        X_train/=255\n",
    "        X_test/=255\n",
    "        \n",
    "        X_train2 = X_train2.reshape(X_train2.shape[0], 224, 224, 1)\n",
    "        X_test2 = X_test2.reshape(X_test2.shape[0], 224, 224, 1)\n",
    "\n",
    "        X_train2 = X_train2.astype('float32')\n",
    "        X_test2 = X_test2.astype('float32')\n",
    "        \n",
    "        X_train2/=255\n",
    "        X_test2/=255\n",
    "        \n",
    "        \n",
    "        number_of_classes = 2\n",
    "        y_train = np_utils.to_categorical(y_train, number_of_classes)\n",
    "        y_test = np_utils.to_categorical(y_test, number_of_classes)\n",
    "        \n",
    "        return X_train, X_train2, y_train, X_test, X_test2, y_test\n",
    "    \n",
    "#     def _train(self, X_train, X_train2, y_train, X_test, X_test2, y_test):\n",
    "    def _train(self, X_train, y_train, X_test, y_test):\n",
    "        seed(1)\n",
    "        self.model.compile(loss=self.loss_function,\n",
    "              optimizer=self.optimizer,\n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "        fit_output = self.model.fit([X_train],\n",
    "                        y_train,\n",
    "                        batch_size=self.batch_size,\n",
    "                        epochs=self.epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=([X_test], y_test))\n",
    "        self._history = fit_output.history\n",
    "        \n",
    "        import time\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.model.save(\"oasis_test_{0}.h5\".format(timestr)) \n",
    "    \n",
    "    @property\n",
    "    def history(self):\n",
    "        return self._history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 166 samples, validate on 50 samples\n",
      "Epoch 1/30\n",
      "166/166 [==============================] - 9s 54ms/step - loss: 1.0047 - acc: 0.5482 - val_loss: 1.1488 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "166/166 [==============================] - 9s 52ms/step - loss: 0.6855 - acc: 0.6566 - val_loss: 0.7899 - val_acc: 0.5000\n",
      "Epoch 3/30\n",
      "166/166 [==============================] - 8s 49ms/step - loss: 0.6544 - acc: 0.6506 - val_loss: 0.7887 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "166/166 [==============================] - 9s 52ms/step - loss: 0.6497 - acc: 0.6506 - val_loss: 0.6976 - val_acc: 0.5000\n",
      "Epoch 5/30\n",
      "166/166 [==============================] - 8s 48ms/step - loss: 0.6578 - acc: 0.6506 - val_loss: 0.6947 - val_acc: 0.5000\n",
      "Epoch 6/30\n",
      "166/166 [==============================] - 8s 49ms/step - loss: 0.7082 - acc: 0.6506 - val_loss: 0.6984 - val_acc: 0.5000\n",
      "Epoch 7/30\n",
      "166/166 [==============================] - 8s 50ms/step - loss: 0.6576 - acc: 0.6506 - val_loss: 1.0145 - val_acc: 0.5000\n",
      "Epoch 8/30\n",
      "166/166 [==============================] - 9s 53ms/step - loss: 0.7306 - acc: 0.6506 - val_loss: 0.7169 - val_acc: 0.5000\n",
      "Epoch 9/30\n",
      "166/166 [==============================] - 9s 53ms/step - loss: 0.6447 - acc: 0.6506 - val_loss: 1.4150 - val_acc: 0.5000\n",
      "Epoch 10/30\n",
      "166/166 [==============================] - 8s 51ms/step - loss: 0.6636 - acc: 0.6506 - val_loss: 0.7017 - val_acc: 0.5000\n",
      "Epoch 11/30\n",
      "166/166 [==============================] - 9s 52ms/step - loss: 0.6452 - acc: 0.6506 - val_loss: 0.6959 - val_acc: 0.5000\n",
      "Epoch 12/30\n",
      "166/166 [==============================] - 9s 56ms/step - loss: 0.6563 - acc: 0.6506 - val_loss: 0.7680 - val_acc: 0.5000\n",
      "Epoch 13/30\n",
      "166/166 [==============================] - 9s 55ms/step - loss: 0.6294 - acc: 0.6506 - val_loss: 0.8089 - val_acc: 0.5000\n",
      "Epoch 14/30\n",
      "166/166 [==============================] - 10s 61ms/step - loss: 0.6287 - acc: 0.6506 - val_loss: 1.3971 - val_acc: 0.5000\n",
      "Epoch 15/30\n",
      "166/166 [==============================] - 9s 57ms/step - loss: 0.6610 - acc: 0.6506 - val_loss: 1.3535 - val_acc: 0.5000\n",
      "Epoch 16/30\n",
      "166/166 [==============================] - 9s 55ms/step - loss: 0.6343 - acc: 0.6747 - val_loss: 0.7304 - val_acc: 0.5000\n",
      "Epoch 17/30\n",
      "166/166 [==============================] - 10s 60ms/step - loss: 0.6040 - acc: 0.6506 - val_loss: 1.2941 - val_acc: 0.5000\n",
      "Epoch 18/30\n",
      "166/166 [==============================] - 10s 62ms/step - loss: 0.6035 - acc: 0.6807 - val_loss: 1.5124 - val_acc: 0.5000\n",
      "Epoch 19/30\n",
      "166/166 [==============================] - 10s 59ms/step - loss: 0.5746 - acc: 0.6747 - val_loss: 0.6626 - val_acc: 0.6200\n",
      "Epoch 20/30\n",
      "166/166 [==============================] - 12s 70ms/step - loss: 0.5563 - acc: 0.7108 - val_loss: 0.6879 - val_acc: 0.5200\n",
      "Epoch 21/30\n",
      "166/166 [==============================] - 15s 88ms/step - loss: 0.5987 - acc: 0.6867 - val_loss: 0.9588 - val_acc: 0.5200\n",
      "Epoch 22/30\n",
      "166/166 [==============================] - 16s 98ms/step - loss: 0.5452 - acc: 0.7470 - val_loss: 1.5197 - val_acc: 0.5000\n",
      "Epoch 23/30\n",
      "166/166 [==============================] - 15s 90ms/step - loss: 0.5598 - acc: 0.7711 - val_loss: 1.8654 - val_acc: 0.5000\n",
      "Epoch 24/30\n",
      "166/166 [==============================] - 15s 88ms/step - loss: 0.5611 - acc: 0.7229 - val_loss: 1.0543 - val_acc: 0.4800\n",
      "Epoch 25/30\n",
      "166/166 [==============================] - 16s 97ms/step - loss: 0.5361 - acc: 0.7590 - val_loss: 1.0652 - val_acc: 0.5000\n",
      "Epoch 26/30\n",
      "166/166 [==============================] - 16s 98ms/step - loss: 0.3843 - acc: 0.8072 - val_loss: 2.4201 - val_acc: 0.5000\n",
      "Epoch 27/30\n",
      "166/166 [==============================] - 15s 93ms/step - loss: 0.4795 - acc: 0.7771 - val_loss: 1.3334 - val_acc: 0.5000\n",
      "Epoch 28/30\n",
      "166/166 [==============================] - 15s 91ms/step - loss: 0.3976 - acc: 0.8133 - val_loss: 2.6228 - val_acc: 0.5000\n",
      "Epoch 29/30\n",
      "166/166 [==============================] - 14s 85ms/step - loss: 0.5901 - acc: 0.7892 - val_loss: 1.2947 - val_acc: 0.5400\n",
      "Epoch 30/30\n",
      "166/166 [==============================] - 9s 55ms/step - loss: 0.2208 - acc: 0.9096 - val_loss: 0.8910 - val_acc: 0.6000\n"
     ]
    }
   ],
   "source": [
    "classifier = LeNetMnistClassifier(X_train=X_train,epochs=30, optimizer=keras.optimizers.Adadelta())\n",
    "classifier.preprocess_and_train(X_train=X_train,y_train=Y_train, X_test=X_test, y_test=Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Dense at 0x10d222eb8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras.models import load_model\n",
    "# model_0p6 = load_model('0p6_lenet.h5')\n",
    "# model_0p6.layers.pop()\n",
    "# # model_0p6.summary()\n",
    "# model_0p6.build()\n",
    "# model_0p6.save('0p6_lenet_pop.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "intermediate_layer = Model(inputs=model_0p6.input, outputs=model_0p6.get_layer(index=12).output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_layer = Model(inputs=model_0p6.input, outputs=model_0p6.get_layer(index=11).output)\n",
    "# print(intermediate_layer)\n",
    "intermediate_output = intermediate_layer.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_output_test = intermediate_layer.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 512)\n"
     ]
    }
   ],
   "source": [
    "print(intermediate_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "\n",
    "demog = pd.read_csv(fn)\n",
    "\n",
    "# print(demog)\n",
    "\n",
    "# demogDF = pd.concat((pd.DataFrame(intermediate_output), demog[['X_sex_binary', 'X_age', 'X_education',\n",
    "#                                                  'X_SES', 'X_eTIV', 'X_nWBV', 'X_ASF', \n",
    "#                                                  'Y_MMSE']]), axis=1)\n",
    "subdemog = demog[demog['Subject'].isin(data.y_train0_id)].reset_index()\n",
    "demogDF = pd.concat((pd.DataFrame(intermediate_output), subdemog[['X_sex_binary', 'X_age','X_eTIV', 'X_nWBV', 'X_ASF', \n",
    "                                                 'Y_MMSE']]), axis=1)\n",
    "metaTrain = np.asmatrix(subdemog[['X_sex_binary', 'X_age','X_eTIV', 'X_nWBV', 'X_ASF', \n",
    "                                                 'Y_MMSE']])\n",
    "demogDFmat = np.asmatrix(demogDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.00000e+00  7.00000e+01  1.28285e+03  7.91000e-01  1.36805e+00\n",
      "   2.90000e+01]]\n"
     ]
    }
   ],
   "source": [
    "print(metaTrain[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "best = 0\n",
    "best_c = -10\n",
    "metaTrainNorm = normalize(metaTrain)#normalize(demogDFmat)\n",
    "metaTestNorm = normalize(metaTest)#normalize(demogDFmattest)\n",
    "\n",
    "for i in np.logspace(4,15,50):\n",
    "    clf = svm.SVC(C=i)\n",
    "    clf.fit(metaTrainNorm, np.asarray(Y_train0))\n",
    "    if(clf.score(metaTestNorm,Y_test)>best):\n",
    "        best = clf.score(metaTestNorm,Y_test)\n",
    "        best_c = i\n",
    "print(best,np.log10(best_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 1e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best,best_c)\n",
    "clf.predict(demogDFmattest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1f59286ce78c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                  'Y_MMSE']])\n\u001b[1;32m      6\u001b[0m \u001b[0mdemogDFmattest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemogDFtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetaTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# print(subdemogtest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "subdemogtest = demog[demog['Subject'].isin(data.y_test0_id)].reset_index()\n",
    "demogDFtest = pd.concat((pd.DataFrame(intermediate_output_test), subdemogtest[['X_sex_binary', 'X_age','X_eTIV', 'X_nWBV', 'X_ASF', \n",
    "                                                 'Y_MMSE']]), axis=1)\n",
    "metaTest = np.asmatrix(subdemogtest[['X_sex_binary', 'X_age','X_eTIV', 'X_nWBV', 'X_ASF', \n",
    "                                                 'Y_MMSE']])\n",
    "demogDFmattest = np.asmatrix(demogDFtest)\n",
    "clf.predict(metaTest)\n",
    "# print(subdemogtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf.score(metaTest,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 166 samples, validate on 50 samples\n",
      "Epoch 1/30\n",
      "166/166 [==============================] - 10s 58ms/step - loss: 0.9702 - acc: 0.5241 - val_loss: 0.6998 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "166/166 [==============================] - 10s 58ms/step - loss: 0.6923 - acc: 0.6506 - val_loss: 0.6951 - val_acc: 0.5000\n",
      "Epoch 3/30\n",
      "166/166 [==============================] - 9s 54ms/step - loss: 0.6686 - acc: 0.6506 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "166/166 [==============================] - 9s 51ms/step - loss: 0.6688 - acc: 0.6506 - val_loss: 0.6939 - val_acc: 0.5000\n",
      "Epoch 5/30\n",
      "166/166 [==============================] - 8s 48ms/step - loss: 0.6594 - acc: 0.6446 - val_loss: 1.0612 - val_acc: 0.5000\n",
      "Epoch 6/30\n",
      "166/166 [==============================] - 8s 51ms/step - loss: 0.6788 - acc: 0.6506 - val_loss: 0.9073 - val_acc: 0.5000\n",
      "Epoch 7/30\n",
      "166/166 [==============================] - 8s 49ms/step - loss: 0.6418 - acc: 0.6506 - val_loss: 0.6924 - val_acc: 0.5000\n",
      "Epoch 8/30\n",
      "166/166 [==============================] - 8s 48ms/step - loss: 0.6595 - acc: 0.6566 - val_loss: 0.8039 - val_acc: 0.5000\n",
      "Epoch 9/30\n",
      "166/166 [==============================] - 8s 50ms/step - loss: 0.6580 - acc: 0.6506 - val_loss: 0.7402 - val_acc: 0.5000\n",
      "Epoch 10/30\n",
      "166/166 [==============================] - 8s 50ms/step - loss: 0.6321 - acc: 0.6506 - val_loss: 0.8482 - val_acc: 0.5000\n",
      "Epoch 11/30\n",
      "166/166 [==============================] - 8s 50ms/step - loss: 0.6145 - acc: 0.6687 - val_loss: 0.6873 - val_acc: 0.5200\n",
      "Epoch 12/30\n",
      "166/166 [==============================] - 8s 49ms/step - loss: 0.6261 - acc: 0.6265 - val_loss: 1.6612 - val_acc: 0.5000\n",
      "Epoch 13/30\n",
      "166/166 [==============================] - 8s 47ms/step - loss: 0.6407 - acc: 0.6747 - val_loss: 0.8120 - val_acc: 0.5400\n",
      "Epoch 14/30\n",
      "166/166 [==============================] - 8s 51ms/step - loss: 0.5761 - acc: 0.6747 - val_loss: 3.6467 - val_acc: 0.5000\n",
      "Epoch 15/30\n",
      "166/166 [==============================] - 9s 55ms/step - loss: 0.7690 - acc: 0.7048 - val_loss: 0.6498 - val_acc: 0.6000\n",
      "Epoch 16/30\n",
      "166/166 [==============================] - 8s 47ms/step - loss: 0.5422 - acc: 0.7289 - val_loss: 1.3512 - val_acc: 0.5000\n",
      "Epoch 17/30\n",
      "166/166 [==============================] - 8s 48ms/step - loss: 0.5572 - acc: 0.7349 - val_loss: 1.3145 - val_acc: 0.5000\n",
      "Epoch 18/30\n",
      "166/166 [==============================] - 8s 48ms/step - loss: 0.5752 - acc: 0.7108 - val_loss: 0.7192 - val_acc: 0.5000\n",
      "Epoch 19/30\n",
      "166/166 [==============================] - 8s 48ms/step - loss: 0.4654 - acc: 0.7651 - val_loss: 0.7551 - val_acc: 0.6200\n",
      "Epoch 20/30\n",
      "166/166 [==============================] - 8s 49ms/step - loss: 0.4479 - acc: 0.7892 - val_loss: 1.5018 - val_acc: 0.5200\n",
      "Epoch 21/30\n",
      "166/166 [==============================] - 8s 50ms/step - loss: 0.4370 - acc: 0.7952 - val_loss: 1.7623 - val_acc: 0.5000\n",
      "Epoch 22/30\n",
      "166/166 [==============================] - 8s 49ms/step - loss: 0.5461 - acc: 0.7831 - val_loss: 0.8471 - val_acc: 0.6200\n",
      "Epoch 23/30\n",
      "166/166 [==============================] - 8s 49ms/step - loss: 0.4471 - acc: 0.7952 - val_loss: 0.6989 - val_acc: 0.6000\n",
      "Epoch 24/30\n",
      "166/166 [==============================] - 8s 49ms/step - loss: 0.3183 - acc: 0.8675 - val_loss: 1.1724 - val_acc: 0.6000\n",
      "Epoch 25/30\n",
      "166/166 [==============================] - 8s 51ms/step - loss: 0.2363 - acc: 0.9277 - val_loss: 1.1893 - val_acc: 0.5000\n",
      "Epoch 26/30\n",
      "166/166 [==============================] - 8s 49ms/step - loss: 0.2070 - acc: 0.9036 - val_loss: 0.9849 - val_acc: 0.6200\n",
      "Epoch 27/30\n",
      "166/166 [==============================] - 8s 49ms/step - loss: 0.1064 - acc: 0.9699 - val_loss: 1.8011 - val_acc: 0.5400\n",
      "Epoch 28/30\n",
      "166/166 [==============================] - 8s 48ms/step - loss: 0.0527 - acc: 0.9759 - val_loss: 2.7028 - val_acc: 0.5600\n",
      "Epoch 29/30\n",
      "166/166 [==============================] - 8s 48ms/step - loss: 0.2866 - acc: 0.9157 - val_loss: 1.4541 - val_acc: 0.6000\n",
      "Epoch 30/30\n",
      "166/166 [==============================] - 8s 51ms/step - loss: 0.0155 - acc: 1.0000 - val_loss: 1.7932 - val_acc: 0.5400\n"
     ]
    }
   ],
   "source": [
    "classifier = LeNetMnistClassifier(epochs=30, optimizer=keras.optimizers.Adadelta())\n",
    "classifier.preprocess_and_train(X_train=X_train, y_train=Y_train, X_test=X_test, y_test=Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13154147634325830913\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 104136704\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 18100578659675994095\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0\"\n",
      "]\n",
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare data set\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
